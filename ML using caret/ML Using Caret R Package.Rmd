---
title: 'Machine Learning Using caret R Package '
output:
  html_notebook:
    theme: united
    toc: yes
  html_document:
    toc: yes
---

***

**Caret R** package automates supervised learning (a.k.a predictive modeling). The primary goal of caret **Don't Overfit**.  

# Supervised Learning

There is two types of predictive models

* **Regression**, that predeicts quantative variables
* **Classification**, that predicts qualitative variables

***

## Linier Regression 

* In regression models, the Root Mean Square Error (**RMSE**) is used to evaluate the model performance by minimizing its value. 
* It is common to calculate in-sample RMSE, however it is too optimistic and usually leads to overfitting. A better approch to calculate out-sample error. 


### In-sample RMSE

```{r results='hold'}
library(tidyverse)
```

```{r}
model <- lm(price ~ ., diamonds)
p <- predict(model, diamonds)
error <- p - diamonds$price

sqrt(mean(error^2))
```
In-sample validation almost gurantees overfitting 

### Out-of-sample Error

To make sure that the models don't overfit and generalize well, we need to test on new data or a *test* set.

```{r}
set.seed(42)
# Shuffle row indices: rows
rows <- sample(nrow(diamonds))
diamonds <- diamonds[rows, ]

split <- round(nrow(diamonds) * .80)
train <- diamonds[1:split, ]
test <- diamonds[(split+1):nrow(diamonds), ]

model <- lm(price ~ ., train)
p <- predict(model, test)
error = p - test$price
sqrt(mean((error)^2))

```
In the previous example we split the data into two sets (training, and test) and calculated the RMSE only once. Based on the spliting sets, we can have another RMSE value. A better approach is cross-validation in which we use multiple test sets which gives more precise estimate for the out-of-sample error.

### Cross-validation 

Test rows are randomly assigned to N folds, each fold represent a different test set. Then, the out-of-samples error will be calculated for each fold to get more accurate estimate. 

**Caret Package** enable us to automate the process of cross-validation as well as it provide a standard interface to apply handrads of different models using the method argument in the train function.

```{r results='hold'}
library(caret)
```

```{r}
model <- train(
  price ~ ., diamonds,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 10, # 10 folds cross-validation(cv)
    #repeats = 5,
    verboseIter = FALSE # don't print the prgress
  )
)

model

p <- predict(model, diamonds)
error = p - diamonds$price
sqrt(mean((error)^2))

```

Repeated cross-validation gives a better estimate of the test-set error. This takes longer, but gives many more out-of-sample datasets to look at and much more precise assessments of how well the model performs.

*** 

## Classification (Logistic Regression)

Logistic regression model is used to predict a categorical (qualitative) target variable (a.k.a churn Yes/No)

```{r results='hold'}
library(mlbench)
data(Sonar)
```

### Confusion Metrix 

[Confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) is a useful tool to evaluate binary classification model.

 ....           | Actual Yes              | Actual No
-------------   | -------------           | -------------
Predection Yes  | `True` Positive (**TP**)| False Positive (**FP**)
Predection No   | False Negative (**FN**) | `Ture` Negative (**TN**)


```{r}
# Shuffle row indices: rows
rows <- sample(nrow(Sonar))
Sonar <- Sonar[rows, ]

split <- round(nrow(Sonar) * 0.6)
train <- Sonar[1:split, ]
test <- Sonar[(split+1): nrow(Sonar), ]

# Fit glm model: model
model <- glm(Class ~ ., family = "binomial", train)
p <- predict(model, test, type = "response")

# Calculate class probabilities: p_class
p_class <- ifelse(p > 0.5, "M", "R")
p_class <- factor(p_class, levels = levels(test$Class))

# Create confusion matrix
confusionMatrix(p_class, test$Class)
```

In the previous example, a 50% threshold has been used. However, we can use different threshold to increase the prediction accuracy 

* 10% would catch more M with less certainty 
* 90% would catch fewer M with more certainty

Unfortunately, there is no good heuristics for choosing prediction threshold ahead of time. Usually, you have to use a confusion matrix and test to find a good threshold.

### ROC Curve 

Manually evaluation for classification models is very hard, it needs to calculate hundreds of confusion matrixes. ROC curve is a systematic approach to let computer evaluate every possible threshold, then plot the true/false positive rates for each. ROC stands for Receiver Operating Characteristic.

```{r}
library(caTools)
colAUC(p, test$Class, plotROC = TRUE)
```

* X-axis: false poistive rate
* Y-axis: true positive rate
* Each point on the curve represents a different threshold (0% : 100%)

### Area Under the Curve (AUC)

Area under the ROC curve is a single number summary of model accuracy. It summarizes performance across all thresholds. AUC for a perfect model is exactly one. AUC for a random model is 0.5. Rule of thumb: AUC as a letter grade 0.9 = "A", 0.8 = "B", 0.7 = "C", 0.5 = "F"

```{r results='hide'}
myControl <- trainControl(
  method = "cv",
  number = 10,
  #To let caret use AUC in evaluating the model performance, use twoClassSummary() function and set classProbs = TRUE
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = FALSE
)

 # Train glm with custom trainControl: model
 model <- train(Class ~ .,
                Sonar, method = "glm",
                trControl = myControl)
```

```{r}
 model
```

### Random Forest 

Non-linear classification models are more robust to overfitting and yield to very accurate results. Unlike linear models, they have hyperparameters thar require manual specification by the data scientist. The default values are often OK, but occasionally need adjustment.


```{r}
set.seed(42)
# Fit a model
model <- train(Class~.,
 data = Sonar,
 method = "ranger",
 #tuneLength = 3,
 trControl = myControl #trainControl(method = "cv", number = 5, verboseIter = FALSE)
 )
# Plot the results
plot(model)
model
```

#### Custom Tuning Grids

To customize caret models, pass custom tuning grids to tuneGrid argument in train method. It gives a complete control over how model is fit. However, it required the most knowledge of how the model works and it can dramatically increase run time.

##### Random Forest Custom Tuning (mtry)

```{r results='hide'}
# Define a custom tuning grid
myGrid <- data.frame(mtry = c(2, 3, 4, 5, 10, 20), 
                     splitrule = "variance",
                     min.node.size = 5)
# Fit a model with a custom tuning grid
set.seed(42)
model <- train(Class ~ ., data = Sonar, method = "ranger",
 tuneGrid = myGrid)

plot(model)
```

### Glmnet

glmnet model is an extantion from glm model with built-in variable selection. It helps in dealing with [collinearity](https://www.quora.com/In-statistics-what-is-the-difference-between-collinearity-and-correlation) and small sample sizes.

There are two types of glmnet model:

* **Lasso regression** [alpha = 0] which penalizes number of non-zero coefficients
* **Ridge regression** [alpha = 1] which penalizes absolute magnitude of coefficients

glmnet can fit a mix of the two models [alpha 0:1], the size of the penaity is another tuning paramter called lambda = 0:Inf 

In general, glmnet model attempts to find a simple model that pairs well with random forest models.


```{r}
# Load data
overfit <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_1048/datasets/overfit.csv") 

# Fit glmnet model: model
model <- train(
  y ~ ., overfit,
  method = "glmnet",
  trControl = myControl
)

model
# Print maximum ROC statistic
max(model[["results"]]$ROC)
```

#### Glmnet & Custom Tuning 

Glmnet has two tunning parameters; alpha and lambda. For each single alpha, all values of lambda fit simultaneously.

```{r}
# Train glmnet with custom trainControl and tuning: model
model <- train(
  y ~ ., overfit,
  tuneGrid = expand.grid(alpha = 0:1, lambda = seq(0.0001, 1, length = 20)),
  method = "glmnet",
  trControl = myControl
)

plot(model)

# Print model to console
model

# Print maximum ROC statistic
max(model[["results"]]$ROC)
```

# Data Preprocessing Using caret

## Dealing with missing values

Most models require numbers, and can't handle missing data. 

The common approach is removing rows with missing data but it can lead to biases in data and generate over-confident models. A better way is median imputation. The median approach works well if data missing at random (**MAR**) and it can be done by passing `preProcess = "medianImpute"` to the train function.

```{r}
# Generate data with missing values
data(mtcars)
mtcars[mtcars$disp < 140, "hp"] <- NA
Y <- mtcars$mpg
X <- mtcars[, 2:4]

# Use median imputation
set.seed(42)
model <- train(x = X, y = Y, method = "glm",
 preProcess = "medianImpute")

print(min(model$results$RMSE))
```

In the previous example, the smaller cars don't report horsepower [NA]. Assuming small cars have medium-large horepower will produce incorrect result.

If the missing data is not missing at random, the median imputation will produce incorrect results. In this case, Random Forest will overcome this challenge or using another approach to handle the misisng values like k-nearest neighbors (**KNN**) imputation that imputes based on "similar" non-missing rows.

```{r}
# Use KNN imputation
set.seed(42)
model <- train(x = X, y = Y,
 method = "glm",
 preProcess = "knnImpute"
 )
print(min(model$results$RMSE)) 
```

## Multiple Preprocessing methods

caret enables us to chain together multiple preprocessing steps. A common recipe for linear models `Median Imputation -> center -> scale -> fit glem` (**order matters**)

Preproceesing summary:

* Start with median imputation. Try KNN imputation if data missing not at random.
* For linear models (`lm, glm, glmnet`)
    + Center and scale 
    + Try PCA and spatial sign
* Tree-based models don't need much preprocessing (just median imputation is enough)

```{r}
# Use linear model "recipe"
set.seed(42)
model <- train(
 x = X, y = Y, method = "glm",
 preProcess = c("knnImpute", "center", "scale", "pca")
 )
print(min(model$results$RMSE))
```

## Handling low-information predictors 

Some variables don't contain much information; contant with no variance or nearly constant with low variance. Sometimes it is useful to remove them perior modeling.

```{r}
# Add constant-valued column to mtcars
X$bad <- 1

set.seed(42)
# Try to fit a model with PCA + glm
model <- train(
 x = X, y = Y, method = "glm",
 #preProcess = c("knnImpute", "center", "scale", "pca")
 preProcess = c("zv", "knnImpute", "center", "scale", "pca")
 )

print(min(model$results$RMSE))

```

Without removing the zero-variance variables, all the matrix will be missing due to the constant variable column which has a zero standard devision. When we try to scale by dividing by **sd**.

**Removing the near zero variance predictors manually** 

```{r}
remove_cols <- nearZeroVar(X, names = TRUE, 
                           freqCut = 2, uniqueCut = 20)
# Get all column names 
all_cols <- names(X)
# Remove from data: bloodbrain_x_small
X_small <- X[ , setdiff(all_cols, remove_cols)]
```

# Compare Multiple Models Using caret

Using `resamples()` to compare multiple models and select (or ensemble) the nest one(s). To compare apple to apple, we need to specify the cross validation folds as well as the trainControl object.

```{r}
library(C50)
data(churn)

# Create train/test indexes
set.seed(42)
myFolds <- createFolds(churnTrain$churn, k = 5)

# Create reusable trainControl object: myControl
myControl <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = FALSE,
  savePredictions = TRUE,
  index = myFolds
)
```

## Glmnet 

Glmnet has the following advantages:

* Fits quickly 
* Ingnores noisy variables 
* Provide interpretable coefficients

```{r}
set.seed(42)
model_glmnet <- train(
 churn ~ ., churnTrain,
 metric = "ROC",
 method = "glmnet",
 tuneGrid = expand.grid(
 alpha = 0:1,
 lambda = 0:10/10
 ),
 trControl = myControl
)
# Plot the results
plot(model_glmnet)
```

## Random Forest 

* Slower to fit then glmnet 
* Less interpretable 
* Often (but not always) more accurate than glmnet 
* Easier to tune

```{r}
set.seed(42)
# Fit random forest: model_rf
churnTrain$churn <- factor(churnTrain$churn, levels = c("no", "yes"))
model_rf <- train(
 churn ~ ., churnTrain,
 metric = "ROC",
 method = "ranger",
 trControl = myControl
)

plot(model_rf)

```

## Gredient Boosting 

```{r}
model_gbm <- train(
 churn ~ ., churnTrain,
 metric = "ROC",
 method = "gbm",
 trControl = myControl
)

plot(model_gbm)
```

## Suport Vector Machine 

```{r}
set.seed(42)
model_svm <- train(
 churn ~ ., churnTrain,
 metric = "ROC",
 method = "svmRadial",
 trControl = myControl
)

plot(model_svm)

```

## Recursive Partitioning Trees 

```{r}
model_rpart <- train(
 churn ~ ., churnTrain,
 metric = "ROC",
 method = "rpart",
 trControl = myControl
)

plot(model_rpart)
```


## Comparing Models

Make sure they were fit on the same data. Apply the following selection criteria 

* Highest average AUC
* Lowest standard deviation in AUC

caret is powerful in assessing the best model using the resamples() method.

```{r}
# Make a list
model_list <- list(
 glmnet = model_glmnet,
 rf = model_rf,
 gbm = model_gbm,
 svm = model_svm, 
 rpart = model_rpart
 )

# Collect resamples from the CV folds
resamps <- resamples(model_list)
resamps 

# Summarize the results
summary(resamps)

dotplot(resamps, metric = "ROC")
```

Random Forest model is bringing the best results. It has the hieghst average AUC as well as the lowest standard devision.

***
